{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import Counter\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import gensim\n",
    "import os, re, csv, math, codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Bidirectional, LSTM, Attention, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Dense, Lambda\n",
    "from keras.models import Sequential,Model\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=Tokenizer(num_words=30000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', lower=True, split=' ', char_level=False, oov_token=\"UNK\", document_count=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "token.fit_on_texts(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences=token.texts_to_sequences(newsgroups_train.data)\n",
    "test_sequences=token.texts_to_sequences(newsgroups_test.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=500\n",
    "train_sequences=pad_sequences(train_sequences,maxlen=max_len)\n",
    "test_sequences=pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_dictionary = token.index_word\n",
    "dictionary = dict([(value, key) for (key, value) in reverse_dictionary.items()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_path = 'glove.6B.300d.txt'\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimensions\n",
    "num_words = len(dictionary) + 1  # Add 1 for the padding token (if used)\n",
    "embed_dim = 300  # Change this to match the dimension of your GloVe embeddings\n",
    "\n",
    "# Create an embedding matrix for your vocabulary\n",
    "embedding_matrix = np.zeros((num_words, embed_dim))\n",
    "\n",
    "# Fill the embedding matrix with GloVe embeddings\n",
    "for word, idx in dictionary.items():\n",
    "    if idx < num_words and word in embeddings_index:\n",
    "        embedding_vector = embeddings_index[word]\n",
    "        embedding_matrix[idx] = embedding_vector\n",
    "    else:\n",
    "        # Handle out-of-vocabulary words here (e.g., use random initialization or zeros)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\keras\\optimizers\\legacy\\adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, Conv1D, Dense, Input, Concatenate,Dot,RepeatVector,TimeDistributed,Multiply,Lambda,Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Activation, MaxPooling1D, GlobalMaxPooling1D, Dropout, Reshape\n",
    "import keras.backend as K\n",
    "from keras.activations import softmax\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "value_dim=100\n",
    "def softMaxOverTime(x):\n",
    "    return softmax(x,axis=1)\n",
    "\n",
    "nb_words=num_words\n",
    "num_filters=64\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer=Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
    "conv_out=Conv1D(value_dim,8,padding=\"same\")(embedding_layer)\n",
    "conv_out=Activation(\"relu\")(conv_out)\n",
    "conv_out=Conv1D(value_dim,8,activation=\"elu\",padding=\"same\")(conv_out)\n",
    "ulog_attention=Dense(1,activation=\"linear\")(conv_out)\n",
    "attention=Activation(softMaxOverTime)(ulog_attention)\n",
    "repeated_attention=TimeDistributed(RepeatVector(value_dim))(attention)\n",
    "repeated_attention=Reshape([max_len,value_dim])(repeated_attention)\n",
    "weighted_embeddings=Multiply()([repeated_attention,conv_out])\n",
    "embedding_sum = Lambda(lambda x: K.sum(x, axis=1))(weighted_embeddings)\n",
    "dense1=Dense(1000, activation='relu')(embedding_sum)\n",
    "dense1_2=Dense(100, activation='relu')(dense1)\n",
    "dense2=Dense(20, activation='softmax')(dense1_2)\n",
    "model=Model(input_layer , dense2)\n",
    "adam = optimizers.Adam(lr=0.001, beta_1=0.999, beta_2=0.999, epsilon=1e-09, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 300)     40243200    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 500, 100)     240100      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 500, 100)     0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 500, 100)     80100       ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 500, 1)       101         ['conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 500, 1)       0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 500, 100, 1)  0          ['activation_1[0][0]']           \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 500, 100)     0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 500, 100)     0           ['reshape[0][0]',                \n",
      "                                                                  'conv1d_1[0][0]']               \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 100)          0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1000)         101000      ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          100100      ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 20)           2020        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,766,621\n",
      "Trainable params: 523,421\n",
      "Non-trainable params: 40,243,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "37/37 [==============================] - 120s 3s/step - loss: 2.7110 - accuracy: 0.1429 - val_loss: 2.1725 - val_accuracy: 0.2545\n",
      "Epoch 2/20\n",
      "37/37 [==============================] - 108s 3s/step - loss: 1.9132 - accuracy: 0.3518 - val_loss: 1.6932 - val_accuracy: 0.4238\n",
      "Epoch 3/20\n",
      "37/37 [==============================] - 116s 3s/step - loss: 1.5229 - accuracy: 0.4671 - val_loss: 1.3856 - val_accuracy: 0.5214\n",
      "Epoch 4/20\n",
      "37/37 [==============================] - 105s 3s/step - loss: 1.2392 - accuracy: 0.5607 - val_loss: 1.1926 - val_accuracy: 0.5802\n",
      "Epoch 5/20\n",
      "37/37 [==============================] - 98s 3s/step - loss: 1.0701 - accuracy: 0.6197 - val_loss: 1.1095 - val_accuracy: 0.6151\n",
      "Epoch 6/20\n",
      "37/37 [==============================] - 100s 3s/step - loss: 0.9475 - accuracy: 0.6593 - val_loss: 1.0564 - val_accuracy: 0.6403\n",
      "Epoch 7/20\n",
      "37/37 [==============================] - 98s 3s/step - loss: 0.8642 - accuracy: 0.6898 - val_loss: 0.9577 - val_accuracy: 0.6792\n",
      "Epoch 8/20\n",
      "37/37 [==============================] - 98s 3s/step - loss: 0.7807 - accuracy: 0.7184 - val_loss: 0.9211 - val_accuracy: 0.6876\n",
      "Epoch 9/20\n",
      "37/37 [==============================] - 98s 3s/step - loss: 0.7136 - accuracy: 0.7499 - val_loss: 0.9167 - val_accuracy: 0.6986\n",
      "Epoch 10/20\n",
      "37/37 [==============================] - 104s 3s/step - loss: 0.6294 - accuracy: 0.7774 - val_loss: 0.9306 - val_accuracy: 0.7159\n",
      "Epoch 11/20\n",
      "37/37 [==============================] - 98s 3s/step - loss: 0.5730 - accuracy: 0.7986 - val_loss: 0.9072 - val_accuracy: 0.7344\n",
      "Epoch 12/20\n",
      "37/37 [==============================] - 100s 3s/step - loss: 0.5041 - accuracy: 0.8207 - val_loss: 0.9078 - val_accuracy: 0.7357\n",
      "Epoch 13/20\n",
      "37/37 [==============================] - 100s 3s/step - loss: 0.4517 - accuracy: 0.8448 - val_loss: 0.9097 - val_accuracy: 0.7605\n",
      "Epoch 14/20\n",
      "37/37 [==============================] - 99s 3s/step - loss: 0.3944 - accuracy: 0.8641 - val_loss: 0.8745 - val_accuracy: 0.7618\n",
      "Epoch 15/20\n",
      "37/37 [==============================] - 100s 3s/step - loss: 0.3587 - accuracy: 0.8782 - val_loss: 0.8670 - val_accuracy: 0.7746\n",
      "Epoch 16/20\n",
      "37/37 [==============================] - 100s 3s/step - loss: 0.3086 - accuracy: 0.8943 - val_loss: 0.9091 - val_accuracy: 0.7861\n",
      "Epoch 17/20\n",
      "37/37 [==============================] - 99s 3s/step - loss: 0.2751 - accuracy: 0.9045 - val_loss: 0.9721 - val_accuracy: 0.7888\n",
      "Epoch 18/20\n",
      "37/37 [==============================] - 99s 3s/step - loss: 0.2275 - accuracy: 0.9220 - val_loss: 1.0322 - val_accuracy: 0.7848\n",
      "Epoch 19/20\n",
      "37/37 [==============================] - 100s 3s/step - loss: 0.2036 - accuracy: 0.9291 - val_loss: 1.0361 - val_accuracy: 0.7870\n",
      "Epoch 20/20\n",
      "37/37 [==============================] - 100s 3s/step - loss: 0.1769 - accuracy: 0.9360 - val_loss: 1.1375 - val_accuracy: 0.7905\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18b17066b10>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sequences, newsgroups_train.target,batch_size=250,epochs=20,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"CNN+ATTENTION.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\sebas\\Desktop\\ITBA\\actuales\\redes\\tp2_redes.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/sebas/Desktop/ITBA/actuales/redes/tp2_redes.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m K\u001b[39m.\u001b[39mclear_session()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'K' is not defined"
     ]
    }
   ],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Concatenate,Dot,RepeatVector,TimeDistributed,Multiply,Lambda,Bidirectional, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout, Dense, Input\n",
    "import keras.backend as K\n",
    "from keras.activations import softmax\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "value_dim=50\n",
    "\n",
    "def softMaxOverTime(x):\n",
    "    return softmax(x,axis=1)\n",
    "\n",
    "nb_words=num_words\n",
    "num_filters=64\n",
    "input_layer = Input(shape=(max_len,))\n",
    "embedding_layer=Embedding(nb_words, embed_dim, weights=[embedding_matrix], input_length=max_len, trainable=False)(input_layer)\n",
    "lstm_out=Bidirectional(LSTM(value_dim, return_sequences=True,activation=\"sigmoid\"),merge_mode=\"sum\")(embedding_layer)\n",
    "ulog_attention=Dense(1,activation=\"sigmoid\")(lstm_out)\n",
    "attention=Activation(softMaxOverTime)(ulog_attention)\n",
    "repeated_attention=TimeDistributed(RepeatVector(value_dim))(attention)\n",
    "repeated_attention=Reshape([max_len,value_dim])(repeated_attention)\n",
    "weighted_embeddings=Multiply()([repeated_attention,lstm_out])\n",
    "embedding_sum = Lambda(lambda x: K.sum(x, axis=1))(weighted_embeddings)\n",
    "dense1=Dense(1000, activation='relu')(embedding_sum)\n",
    "dense1_2=Dense(100, activation='relu')(dense1)\n",
    "dense2=Dense(20, activation='softmax')(dense1_2)\n",
    "model=Model(input_layer , dense2)\n",
    "#adam = optimizers.Adam(lr=0.001, beta_1=0.999, beta_2=0.999, epsilon=1e-09, decay=0.0)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 500)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 500, 300)     40243200    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 500, 150)     541200      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 500, 1)       151         ['bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 500, 1)       0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 500, 150, 1)  0          ['activation[0][0]']             \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 500, 150)     0           ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " multiply (Multiply)            (None, 500, 150)     0           ['reshape[0][0]',                \n",
      "                                                                  'bidirectional[0][0]']          \n",
      "                                                                                                  \n",
      " lambda (Lambda)                (None, 150)          0           ['multiply[0][0]']               \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 500)          75500       ['lambda[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 100)          50100       ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 20)           2020        ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 40,912,171\n",
      "Trainable params: 668,971\n",
      "Non-trainable params: 40,243,200\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_sequences, newsgroups_train.target,batch_size=2000,epochs=20,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"BIDIR+ATTENTION.h5\")\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers redes1 Por si necesito usar en algun momento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"modelo.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=3, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_x, train_y,batch_size=28,epochs=30,validation_split=0.2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_path = 'path/to/x.txt'\n",
    "word_to_vec = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.array(values[1:], dtype='float32')\n",
    "        word_to_vec[word] = vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
